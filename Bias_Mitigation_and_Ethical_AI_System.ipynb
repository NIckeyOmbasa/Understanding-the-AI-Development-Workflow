{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Bias Mitigation and Ethical AI System**\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**AI Development Workflow Assignment - Part 3 Implementation **\n",
        "\n",
        "This module implements fairness-aware machine learning techniques\n",
        "to address algorithmic bias in healthcare AI systems.\n"
      ],
      "metadata": {
        "id": "fwsK-vu12tbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_classification\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "BDZwQekD3C5c"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygb2sblY2oFl",
        "outputId": "eed1ecaa-2bce-42d0-b2a3-9718e4b41c86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Healthcare AI Bias Audit Demonstration ===\n",
            "\n",
            "Training fairness-aware classifier...\n",
            "\n",
            "Conducting bias audit...\n",
            "Performing comprehensive bias audit...\n",
            "\n",
            "Generating audit report...\n",
            "\n",
            "ALGORITHMIC BIAS AUDIT REPORT\n",
            "============================\n",
            "Generated: 2025-07-25 00:08:06\n",
            "\n",
            "EXECUTIVE SUMMARY\n",
            "-----------------\n",
            "This report presents findings from a comprehensive algorithmic bias audit\n",
            "of the healthcare AI system. The audit examines fairness across demographic\n",
            "groups and provides recommendations for bias mitigation.\n",
            "\n",
            "\n",
            "BIAS ANALYSIS: RACE\n",
            "-------------------\n",
            "\n",
            "Group Performance Metrics:\n",
            "\n",
            "White:\n",
            "  - Sample Size: 285\n",
            "  - Base Rate: 0.354\n",
            "  - Positive Prediction Rate: 0.309\n",
            "  - Precision: 0.739\n",
            "  - Recall: 0.644\n",
            "  - AUC Score: 0.843\n",
            "\n",
            "Asian:\n",
            "  - Sample Size: 110\n",
            "  - Base Rate: 0.218\n",
            "  - Positive Prediction Rate: 0.273\n",
            "  - Precision: 0.533\n",
            "  - Recall: 0.667\n",
            "  - AUC Score: 0.818\n",
            "\n",
            "Black:\n",
            "  - Sample Size: 140\n",
            "  - Base Rate: 0.214\n",
            "  - Positive Prediction Rate: 0.279\n",
            "  - Precision: 0.410\n",
            "  - Recall: 0.533\n",
            "  - AUC Score: 0.766\n",
            "\n",
            "Hispanic:\n",
            "  - Sample Size: 65\n",
            "  - Base Rate: 0.323\n",
            "  - Positive Prediction Rate: 0.323\n",
            "  - Precision: 0.619\n",
            "  - Recall: 0.619\n",
            "  - AUC Score: 0.773\n",
            "\n",
            "Bias Measures:\n",
            "  - Demographic Parity Difference: 0.050\n",
            "  - Equalized Odds Difference: 0.133\n",
            "  - Equality of Opportunity Difference: 0.084\n",
            "  - Calibration Difference: 0.045\n",
            "\n",
            "Bias Severity Assessment: MODERATE - Bias mitigation recommended\n",
            "\n",
            "BIAS ANALYSIS: GENDER\n",
            "---------------------\n",
            "\n",
            "Group Performance Metrics:\n",
            "\n",
            "Female:\n",
            "  - Sample Size: 299\n",
            "  - Base Rate: 0.318\n",
            "  - Positive Prediction Rate: 0.324\n",
            "  - Precision: 0.629\n",
            "  - Recall: 0.642\n",
            "  - AUC Score: 0.799\n",
            "\n",
            "Male:\n",
            "  - Sample Size: 249\n",
            "  - Base Rate: 0.273\n",
            "  - Positive Prediction Rate: 0.273\n",
            "  - Precision: 0.618\n",
            "  - Recall: 0.618\n",
            "  - AUC Score: 0.844\n",
            "\n",
            "Other:\n",
            "  - Sample Size: 52\n",
            "  - Base Rate: 0.250\n",
            "  - Positive Prediction Rate: 0.250\n",
            "  - Precision: 0.538\n",
            "  - Recall: 0.538\n",
            "  - AUC Score: 0.734\n",
            "\n",
            "Bias Measures:\n",
            "  - Demographic Parity Difference: 0.074\n",
            "  - Equalized Odds Difference: 0.104\n",
            "  - Equality of Opportunity Difference: 0.033\n",
            "  - Calibration Difference: 0.024\n",
            "\n",
            "Bias Severity Assessment: MODERATE - Bias mitigation recommended\n",
            "\n",
            "BIAS ANALYSIS: AGE_GROUP\n",
            "------------------------\n",
            "\n",
            "Group Performance Metrics:\n",
            "\n",
            "18-30:\n",
            "  - Sample Size: 188\n",
            "  - Base Rate: 0.309\n",
            "  - Positive Prediction Rate: 0.346\n",
            "  - Precision: 0.600\n",
            "  - Recall: 0.672\n",
            "  - AUC Score: 0.818\n",
            "\n",
            "31-50:\n",
            "  - Sample Size: 210\n",
            "  - Base Rate: 0.305\n",
            "  - Positive Prediction Rate: 0.305\n",
            "  - Precision: 0.641\n",
            "  - Recall: 0.641\n",
            "  - AUC Score: 0.823\n",
            "\n",
            "51+:\n",
            "  - Sample Size: 202\n",
            "  - Base Rate: 0.267\n",
            "  - Positive Prediction Rate: 0.243\n",
            "  - Precision: 0.612\n",
            "  - Recall: 0.556\n",
            "  - AUC Score: 0.792\n",
            "\n",
            "Bias Measures:\n",
            "  - Demographic Parity Difference: 0.103\n",
            "  - Equalized Odds Difference: 0.117\n",
            "  - Equality of Opportunity Difference: 0.072\n",
            "  - Calibration Difference: 0.005\n",
            "\n",
            "Bias Severity Assessment: MODERATE - Bias mitigation recommended\n",
            "\n",
            "BIAS MITIGATION RECOMMENDATIONS\n",
            "==============================\n",
            "\n",
            "Based on the audit findings, the following actions are recommended:\n",
            "\n",
            "1. IMMEDIATE ACTIONS:\n",
            "   - Implement fairness-aware model training with demographic parity constraints\n",
            "   - Establish group-specific prediction thresholds\n",
            "   - Enhance data collection to ensure representative sampling\n",
            "\n",
            "2. SHORT-TERM IMPROVEMENTS:\n",
            "   - Implement post-processing calibration techniques\n",
            "   - Develop bias monitoring dashboards for continuous assessment\n",
            "   - Train clinical staff on bias-aware decision making\n",
            "\n",
            "3. LONG-TERM STRATEGIES:\n",
            "   - Collect more diverse and representative training data\n",
            "   - Implement federated learning approaches across diverse healthcare systems\n",
            "   - Establish regular bias auditing protocols (quarterly assessments)\n",
            "\n",
            "4. REGULATORY COMPLIANCE:\n",
            "   - Document bias mitigation efforts for regulatory review\n",
            "   - Implement explainable AI techniques for clinical transparency\n",
            "   - Establish patient consent protocols for AI-assisted care\n",
            "\n",
            "5. CONTINUOUS MONITORING:\n",
            "   - Set up automated bias detection alerts\n",
            "   - Implement A/B testing for bias mitigation strategies\n",
            "   - Regular retraining with updated diverse datasets\n",
            "\n",
            "\n",
            "Report saved to 'healthcare_ai_bias_audit.txt'\n"
          ]
        }
      ],
      "source": [
        "class FairnessAwareClassifier(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    A fairness-aware classifier that implements demographic parity constraints\n",
        "    and bias mitigation techniques for healthcare AI applications.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_classifier=None, fairness_constraint='demographic_parity',\n",
        "                 lambda_fairness=0.1, sensitive_features=None):\n",
        "        \"\"\"\n",
        "        Initialize fairness-aware classifier.\n",
        "\n",
        "        Args:\n",
        "            base_classifier: Base ML model (default: RandomForestClassifier)\n",
        "            fairness_constraint: Type of fairness constraint ('demographic_parity', 'equalized_odds')\n",
        "            lambda_fairness: Weight for fairness regularization term\n",
        "            sensitive_features: List of sensitive feature column names\n",
        "        \"\"\"\n",
        "        self.base_classifier = base_classifier or RandomForestClassifier(random_state=42)\n",
        "        self.fairness_constraint = fairness_constraint\n",
        "        self.lambda_fairness = lambda_fairness\n",
        "        self.sensitive_features = sensitive_features or []\n",
        "        self.threshold_adjustments = {}\n",
        "        self.is_fitted = False\n",
        "\n",
        "    def fit(self, X, y, sensitive_attributes=None):\n",
        "        \"\"\"\n",
        "        Fit the fairness-aware classifier.\n",
        "\n",
        "        Args:\n",
        "            X: Training features\n",
        "            y: Training target\n",
        "            sensitive_attributes: DataFrame with sensitive attributes\n",
        "        \"\"\"\n",
        "        # Fit base classifier\n",
        "        self.base_classifier.fit(X, y)\n",
        "\n",
        "        if sensitive_attributes is not None:\n",
        "            # Calculate group-specific thresholds for fairness\n",
        "            self._calculate_fair_thresholds(X, y, sensitive_attributes)\n",
        "\n",
        "        self.is_fitted = True\n",
        "        return self\n",
        "\n",
        "    def _calculate_fair_thresholds(self, X, y, sensitive_attributes):\n",
        "        \"\"\"\n",
        "        Calculate group-specific prediction thresholds to ensure fairness.\n",
        "        \"\"\"\n",
        "        # Get base predictions\n",
        "        base_proba = self.base_classifier.predict_proba(X)[:, 1]\n",
        "\n",
        "        # Calculate optimal thresholds for each sensitive group\n",
        "        for sensitive_feature in self.sensitive_features:\n",
        "            if sensitive_feature in sensitive_attributes.columns:\n",
        "                group_thresholds = {}\n",
        "                unique_groups = sensitive_attributes[sensitive_feature].unique()\n",
        "\n",
        "                # Calculate baseline metrics\n",
        "                overall_positive_rate = y.mean()\n",
        "\n",
        "                for group in unique_groups:\n",
        "                    group_mask = sensitive_attributes[sensitive_feature] == group\n",
        "                    group_y = y[group_mask]\n",
        "                    group_proba = base_proba[group_mask]\n",
        "\n",
        "                    if len(group_y) > 0:\n",
        "                        # Find threshold that achieves demographic parity\n",
        "                        if self.fairness_constraint == 'demographic_parity':\n",
        "                            # Adjust threshold to match overall positive prediction rate\n",
        "                            threshold = np.percentile(group_proba, (1 - overall_positive_rate) * 100)\n",
        "                            group_thresholds[group] = max(0.1, min(0.9, threshold))\n",
        "\n",
        "                        elif self.fairness_constraint == 'equalized_odds':\n",
        "                            # Adjust threshold to equalize TPR and FPR across groups\n",
        "                            threshold = self._find_equalized_odds_threshold(group_y, group_proba, overall_positive_rate)\n",
        "                            group_thresholds[group] = threshold\n",
        "\n",
        "                self.threshold_adjustments[sensitive_feature] = group_thresholds\n",
        "\n",
        "    def _find_equalized_odds_threshold(self, y_true, y_proba, target_rate):\n",
        "        \"\"\"\n",
        "        Find threshold that achieves equalized odds constraint.\n",
        "        \"\"\"\n",
        "        thresholds = np.linspace(0.1, 0.9, 50)\n",
        "        best_threshold = 0.5\n",
        "        best_score = float('inf')\n",
        "\n",
        "        for threshold in thresholds:\n",
        "            predictions = (y_proba >= threshold).astype(int)\n",
        "\n",
        "            # Calculate TPR and FPR\n",
        "            tp = np.sum((y_true == 1) & (predictions == 1))\n",
        "            fp = np.sum((y_true == 0) & (predictions == 1))\n",
        "            tn = np.sum((y_true == 0) & (predictions == 0))\n",
        "            fn = np.sum((y_true == 1) & (predictions == 0))\n",
        "\n",
        "            tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "\n",
        "            # Score based on deviation from target rates\n",
        "            score = abs(tpr - target_rate) + abs(fpr - (1 - target_rate))\n",
        "\n",
        "            if score < best_score:\n",
        "                best_score = score\n",
        "                best_threshold = threshold\n",
        "\n",
        "        return best_threshold\n",
        "\n",
        "    def predict(self, X, sensitive_attributes=None):\n",
        "        \"\"\"\n",
        "        Make fairness-aware predictions.\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Model must be fitted before prediction\")\n",
        "\n",
        "        # Get base predictions\n",
        "        base_proba = self.base_classifier.predict_proba(X)[:, 1]\n",
        "\n",
        "        if sensitive_attributes is None or not self.threshold_adjustments:\n",
        "            # Use standard threshold if no fairness adjustments\n",
        "            return (base_proba >= 0.5).astype(int)\n",
        "\n",
        "        # Apply group-specific thresholds\n",
        "        predictions = np.zeros(len(X))\n",
        "\n",
        "        for sensitive_feature, group_thresholds in self.threshold_adjustments.items():\n",
        "            if sensitive_feature in sensitive_attributes.columns:\n",
        "                for group, threshold in group_thresholds.items():\n",
        "                    group_mask = sensitive_attributes[sensitive_feature] == group\n",
        "                    predictions[group_mask] = (base_proba[group_mask] >= threshold).astype(int)\n",
        "\n",
        "        return predictions.astype(int)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Return prediction probabilities from base classifier.\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Model must be fitted before prediction\")\n",
        "\n",
        "        return self.base_classifier.predict_proba(X)\n",
        "\n",
        "\n",
        "class BiasAuditSystem:\n",
        "    \"\"\"\n",
        "    Comprehensive system for detecting and measuring algorithmic bias\n",
        "    in machine learning models used in healthcare.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.audit_results = {}\n",
        "        self.mitigation_strategies = []\n",
        "\n",
        "    def comprehensive_bias_audit(self, model, X_test, y_test, sensitive_attributes,\n",
        "                                sensitive_features=['race', 'gender']):\n",
        "        \"\"\"\n",
        "        Perform comprehensive bias audit across multiple fairness metrics.\n",
        "\n",
        "        Args:\n",
        "            model: Trained ML model\n",
        "            X_test: Test features\n",
        "            y_test: Test target\n",
        "            sensitive_attributes: DataFrame with sensitive attributes\n",
        "            sensitive_features: List of sensitive features to audit\n",
        "\n",
        "        Returns:\n",
        "            dict: Comprehensive bias audit results\n",
        "        \"\"\"\n",
        "        print(\"Performing comprehensive bias audit...\")\n",
        "\n",
        "        # Get predictions\n",
        "        if hasattr(model, 'predict_proba'):\n",
        "            y_proba = model.predict_proba(X_test)[:, 1]\n",
        "        else:\n",
        "            y_proba = model.predict(X_test)\n",
        "\n",
        "        y_pred = model.predict(X_test, sensitive_attributes=sensitive_attributes)\n",
        "\n",
        "        audit_results = {}\n",
        "\n",
        "        for feature in sensitive_features:\n",
        "            if feature in sensitive_attributes.columns:\n",
        "                feature_results = self._audit_feature_bias(\n",
        "                    y_test, y_pred, y_proba, sensitive_attributes[feature], feature\n",
        "                )\n",
        "                audit_results[feature] = feature_results\n",
        "\n",
        "        # Calculate intersectional bias (multiple sensitive attributes)\n",
        "        if len(sensitive_features) > 1:\n",
        "            intersectional_results = self._audit_intersectional_bias(\n",
        "                y_test, y_pred, y_proba, sensitive_attributes, sensitive_features\n",
        "            )\n",
        "            audit_results['intersectional'] = intersectional_results\n",
        "\n",
        "        self.audit_results = audit_results\n",
        "        return audit_results\n",
        "\n",
        "    def _audit_feature_bias(self, y_true, y_pred, y_proba, sensitive_attribute, feature_name):\n",
        "        \"\"\"\n",
        "        Audit bias for a single sensitive feature.\n",
        "        \"\"\"\n",
        "        unique_groups = sensitive_attribute.unique()\n",
        "        group_metrics = {}\n",
        "\n",
        "        for group in unique_groups:\n",
        "            group_mask = sensitive_attribute == group\n",
        "            group_y_true = y_true[group_mask]\n",
        "            group_y_pred = y_pred[group_mask]\n",
        "            group_y_proba = y_proba[group_mask]\n",
        "\n",
        "            if len(group_y_true) > 0:\n",
        "                # Calculate fairness metrics\n",
        "                metrics = self._calculate_fairness_metrics(group_y_true, group_y_pred, group_y_proba)\n",
        "                group_metrics[group] = metrics\n",
        "\n",
        "        # Calculate bias measures\n",
        "        bias_measures = self._calculate_bias_measures(group_metrics)\n",
        "\n",
        "        return {\n",
        "            'group_metrics': group_metrics,\n",
        "            'bias_measures': bias_measures,\n",
        "            'feature_name': feature_name\n",
        "        }\n",
        "\n",
        "    def _calculate_fairness_metrics(self, y_true, y_pred, y_proba):\n",
        "        \"\"\"\n",
        "        Calculate comprehensive fairness metrics for a group.\n",
        "        \"\"\"\n",
        "        # Confusion matrix components\n",
        "        cm = confusion_matrix(y_true, y_pred).ravel()\n",
        "        if len(cm) == 1:\n",
        "            # Handle single-class case\n",
        "            if y_true.iloc[0] == 0:  # All negatives\n",
        "                tn, fp, fn, tp = cm[0], 0, 0, 0\n",
        "            else:  # All positives\n",
        "                tn, fp, fn, tp = 0, 0, 0, cm[0]\n",
        "        else:\n",
        "            tn, fp, fn, tp = cm\n",
        "\n",
        "        # Basic metrics\n",
        "        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "        # Fairness-specific metrics\n",
        "        positive_prediction_rate = np.mean(y_pred)\n",
        "        negative_prediction_rate = 1 - positive_prediction_rate\n",
        "        base_rate = np.mean(y_true)\n",
        "\n",
        "        # Calibration metrics\n",
        "        calibration_score = self._calculate_calibration(y_true, y_proba)\n",
        "\n",
        "        return {\n",
        "            'sample_size': len(y_true),\n",
        "            'base_rate': base_rate,\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'specificity': specificity,\n",
        "            'positive_prediction_rate': positive_prediction_rate,\n",
        "            'negative_prediction_rate': negative_prediction_rate,\n",
        "            'calibration_score': calibration_score,\n",
        "            'auc_score': roc_auc_score(y_true, y_proba) if len(np.unique(y_true)) > 1 else 0.5\n",
        "        }\n",
        "\n",
        "    def _calculate_calibration(self, y_true, y_proba, n_bins=10):\n",
        "        \"\"\"\n",
        "        Calculate calibration score (Brier score).\n",
        "        \"\"\"\n",
        "        return np.mean((y_proba - y_true) ** 2)\n",
        "\n",
        "    def _calculate_bias_measures(self, group_metrics):\n",
        "        \"\"\"\n",
        "        Calculate bias measures comparing groups.\n",
        "        \"\"\"\n",
        "        if len(group_metrics) < 2:\n",
        "            return {}\n",
        "\n",
        "        groups = list(group_metrics.keys())\n",
        "        bias_measures = {}\n",
        "\n",
        "        # Demographic Parity Difference\n",
        "        pprs = [group_metrics[group]['positive_prediction_rate'] for group in groups]\n",
        "        bias_measures['demographic_parity_diff'] = max(pprs) - min(pprs)\n",
        "\n",
        "        # Equalized Odds Difference (TPR difference)\n",
        "        tprs = [group_metrics[group]['recall'] for group in groups]\n",
        "        bias_measures['equalized_odds_diff'] = max(tprs) - min(tprs)\n",
        "\n",
        "        # Equality of Opportunity Difference (FPR difference)\n",
        "        fprs = [1 - group_metrics[group]['specificity'] for group in groups]\n",
        "        bias_measures['equality_opportunity_diff'] = max(fprs) - min(fprs)\n",
        "\n",
        "        # Calibration Difference\n",
        "        calibrations = [group_metrics[group]['calibration_score'] for group in groups]\n",
        "        bias_measures['calibration_diff'] = max(calibrations) - min(calibrations)\n",
        "\n",
        "        # Statistical Significance Tests\n",
        "        bias_measures['statistical_tests'] = self._perform_bias_tests(group_metrics)\n",
        "\n",
        "        return bias_measures\n",
        "\n",
        "    def _perform_bias_tests(self, group_metrics):\n",
        "        \"\"\"\n",
        "        Perform statistical tests for bias detection.\n",
        "        \"\"\"\n",
        "        groups = list(group_metrics.keys())\n",
        "        if len(groups) < 2:\n",
        "            return {}\n",
        "\n",
        "        tests = {}\n",
        "\n",
        "        # Chi-square test for independence (prediction rates)\n",
        "        pprs = [group_metrics[group]['positive_prediction_rate'] for group in groups]\n",
        "        sample_sizes = [group_metrics[group]['sample_size'] for group in groups]\n",
        "\n",
        "        # Perform chi-square test\n",
        "        observed = [[int(ppr * size), int((1-ppr) * size)] for ppr, size in zip(pprs, sample_sizes)]\n",
        "        if all(sum(obs) > 0 for obs in observed):\n",
        "            try:\n",
        "                chi2, p_value = stats.chi2_contingency(observed)[:2]\n",
        "                tests['chi2_test'] = {'statistic': chi2, 'p_value': p_value}\n",
        "            except:\n",
        "                tests['chi2_test'] = {'statistic': None, 'p_value': None}\n",
        "\n",
        "        return tests\n",
        "\n",
        "    def _audit_intersectional_bias(self, y_true, y_pred, y_proba, sensitive_attributes, features):\n",
        "        \"\"\"\n",
        "        Audit intersectional bias across multiple sensitive attributes.\n",
        "        \"\"\"\n",
        "        # Create intersectional groups\n",
        "        intersectional_groups = sensitive_attributes[features].apply(\n",
        "            lambda x: '_'.join(x.astype(str)), axis=1\n",
        "        )\n",
        "\n",
        "        return self._audit_feature_bias(y_true, y_pred, y_proba, intersectional_groups, 'intersectional')\n",
        "\n",
        "    def generate_bias_report(self, audit_results=None):\n",
        "        \"\"\"\n",
        "        Generate comprehensive bias audit report.\n",
        "        \"\"\"\n",
        "        if audit_results is None:\n",
        "            audit_results = self.audit_results\n",
        "\n",
        "        report = \"\"\"\n",
        "ALGORITHMIC BIAS AUDIT REPORT\n",
        "============================\n",
        "Generated: {timestamp}\n",
        "\n",
        "EXECUTIVE SUMMARY\n",
        "-----------------\n",
        "This report presents findings from a comprehensive algorithmic bias audit\n",
        "of the healthcare AI system. The audit examines fairness across demographic\n",
        "groups and provides recommendations for bias mitigation.\n",
        "\n",
        "\"\"\".format(timestamp=pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
        "\n",
        "        for feature, results in audit_results.items():\n",
        "            if feature != 'intersectional':\n",
        "                report += self._format_feature_report(feature, results)\n",
        "\n",
        "        # Add recommendations\n",
        "        report += self._generate_recommendations(audit_results)\n",
        "\n",
        "        return report\n",
        "\n",
        "    def _format_feature_report(self, feature, results):\n",
        "        \"\"\"\n",
        "        Format bias audit results for a specific feature.\n",
        "        \"\"\"\n",
        "        report = f\"\"\"\n",
        "BIAS ANALYSIS: {feature.upper()}\n",
        "{'-' * (15 + len(feature))}\n",
        "\n",
        "Group Performance Metrics:\n",
        "\"\"\"\n",
        "\n",
        "        for group, metrics in results['group_metrics'].items():\n",
        "            report += f\"\"\"\n",
        "{group}:\n",
        "  - Sample Size: {metrics['sample_size']}\n",
        "  - Base Rate: {metrics['base_rate']:.3f}\n",
        "  - Positive Prediction Rate: {metrics['positive_prediction_rate']:.3f}\n",
        "  - Precision: {metrics['precision']:.3f}\n",
        "  - Recall: {metrics['recall']:.3f}\n",
        "  - AUC Score: {metrics['auc_score']:.3f}\n",
        "\"\"\"\n",
        "\n",
        "        # Add bias measures\n",
        "        bias_measures = results['bias_measures']\n",
        "        report += f\"\"\"\n",
        "Bias Measures:\n",
        "  - Demographic Parity Difference: {bias_measures.get('demographic_parity_diff', 0):.3f}\n",
        "  - Equalized Odds Difference: {bias_measures.get('equalized_odds_diff', 0):.3f}\n",
        "  - Equality of Opportunity Difference: {bias_measures.get('equality_opportunity_diff', 0):.3f}\n",
        "  - Calibration Difference: {bias_measures.get('calibration_diff', 0):.3f}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        # Bias severity assessment\n",
        "        severity = self._assess_bias_severity(bias_measures)\n",
        "        report += f\"Bias Severity Assessment: {severity}\\n\"\n",
        "\n",
        "        return report\n",
        "\n",
        "    def _assess_bias_severity(self, bias_measures):\n",
        "        \"\"\"\n",
        "        Assess the severity of detected bias.\n",
        "        \"\"\"\n",
        "        dp_diff = bias_measures.get('demographic_parity_diff', 0)\n",
        "        eo_diff = bias_measures.get('equalized_odds_diff', 0)\n",
        "\n",
        "        if dp_diff > 0.2 or eo_diff > 0.2:\n",
        "            return \"HIGH - Immediate intervention required\"\n",
        "        elif dp_diff > 0.1 or eo_diff > 0.1:\n",
        "            return \"MODERATE - Bias mitigation recommended\"\n",
        "        elif dp_diff > 0.05 or eo_diff > 0.05:\n",
        "            return \"LOW - Monitor and consider mitigation\"\n",
        "        else:\n",
        "            return \"MINIMAL - Continue monitoring\"\n",
        "\n",
        "    def _generate_recommendations(self, audit_results):\n",
        "        \"\"\"\n",
        "        Generate bias mitigation recommendations.\n",
        "        \"\"\"\n",
        "        recommendations = \"\"\"\n",
        "BIAS MITIGATION RECOMMENDATIONS\n",
        "==============================\n",
        "\n",
        "Based on the audit findings, the following actions are recommended:\n",
        "\n",
        "1. IMMEDIATE ACTIONS:\n",
        "   - Implement fairness-aware model training with demographic parity constraints\n",
        "   - Establish group-specific prediction thresholds\n",
        "   - Enhance data collection to ensure representative sampling\n",
        "\n",
        "2. SHORT-TERM IMPROVEMENTS:\n",
        "   - Implement post-processing calibration techniques\n",
        "   - Develop bias monitoring dashboards for continuous assessment\n",
        "   - Train clinical staff on bias-aware decision making\n",
        "\n",
        "3. LONG-TERM STRATEGIES:\n",
        "   - Collect more diverse and representative training data\n",
        "   - Implement federated learning approaches across diverse healthcare systems\n",
        "   - Establish regular bias auditing protocols (quarterly assessments)\n",
        "\n",
        "4. REGULATORY COMPLIANCE:\n",
        "   - Document bias mitigation efforts for regulatory review\n",
        "   - Implement explainable AI techniques for clinical transparency\n",
        "   - Establish patient consent protocols for AI-assisted care\n",
        "\n",
        "5. CONTINUOUS MONITORING:\n",
        "   - Set up automated bias detection alerts\n",
        "   - Implement A/B testing for bias mitigation strategies\n",
        "   - Regular retraining with updated diverse datasets\n",
        "\"\"\"\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "\n",
        "# =============================================\n",
        "# DEMONSTRATION CODE\n",
        "# =============================================\n",
        "\n",
        "def run_demo():\n",
        "    \"\"\"Demonstrate the fairness auditing system with synthetic healthcare data\"\"\"\n",
        "    print(\"=== Healthcare AI Bias Audit Demonstration ===\")\n",
        "\n",
        "    # Generate synthetic healthcare dataset\n",
        "    X, y = make_classification(\n",
        "        n_samples=2000,\n",
        "        n_features=15,\n",
        "        n_informative=8,\n",
        "        n_classes=2,\n",
        "        weights=[0.7, 0.3],  # Imbalanced classes\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
        "    y = pd.Series(y, name='target')\n",
        "\n",
        "    # Create sensitive attributes (race, gender, age_group)\n",
        "    sensitive_attrs = pd.DataFrame({\n",
        "        'race': np.random.choice(['White', 'Black', 'Asian', 'Hispanic'], size=2000, p=[0.5, 0.2, 0.2, 0.1]),\n",
        "        'gender': np.random.choice(['Male', 'Female', 'Other'], size=2000, p=[0.45, 0.5, 0.05]),\n",
        "        'age_group': np.random.choice(['18-30', '31-50', '51+'], size=2000)\n",
        "    })\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    sens_attrs_train = sensitive_attrs.loc[X_train.index]\n",
        "    sens_attrs_test = sensitive_attrs.loc[X_test.index]\n",
        "\n",
        "    # Initialize and train fairness-aware model\n",
        "    print(\"\\nTraining fairness-aware classifier...\")\n",
        "    fair_model = FairnessAwareClassifier(\n",
        "        base_classifier=LogisticRegression(max_iter=1000),\n",
        "        fairness_constraint='demographic_parity',\n",
        "        sensitive_features=['race', 'gender']\n",
        "    )\n",
        "    fair_model.fit(X_train, y_train, sens_attrs_train)\n",
        "\n",
        "    # Perform comprehensive bias audit\n",
        "    print(\"\\nConducting bias audit...\")\n",
        "    auditor = BiasAuditSystem()\n",
        "    audit_results = auditor.comprehensive_bias_audit(\n",
        "        fair_model,\n",
        "        X_test,\n",
        "        y_test,\n",
        "        sens_attrs_test,\n",
        "        sensitive_features=['race', 'gender', 'age_group']\n",
        "    )\n",
        "\n",
        "    # Generate and display report\n",
        "    print(\"\\nGenerating audit report...\")\n",
        "    report = auditor.generate_bias_report(audit_results)\n",
        "    print(report)\n",
        "\n",
        "    # Save report to file\n",
        "    with open(\"healthcare_ai_bias_audit.txt\", \"w\") as f:\n",
        "        f.write(report)\n",
        "    print(\"\\nReport saved to 'healthcare_ai_bias_audit.txt'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_demo()"
      ]
    }
  ]
}